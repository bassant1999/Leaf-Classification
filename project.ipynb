{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "dataset that is used here is https://www.kaggle.com/c/leaf-classification/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset, visualize the data, and describe the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1575</td>\n",
       "      <td>Magnolia_Salicifolia</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1578</td>\n",
       "      <td>Acer_Pictum</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1581</td>\n",
       "      <td>Alnus_Maximowiczii</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>1582</td>\n",
       "      <td>Quercus_Rubra</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1584</td>\n",
       "      <td>Quercus_Afares</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                species   margin1   margin2   margin3   margin4  \\\n",
       "0       1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1       2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2       3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3       5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4       6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "..    ...                    ...       ...       ...       ...       ...   \n",
       "985  1575   Magnolia_Salicifolia  0.060547  0.119140  0.007812  0.003906   \n",
       "986  1578            Acer_Pictum  0.001953  0.003906  0.021484  0.107420   \n",
       "987  1581     Alnus_Maximowiczii  0.001953  0.003906  0.000000  0.021484   \n",
       "988  1582          Quercus_Rubra  0.000000  0.000000  0.046875  0.056641   \n",
       "989  1584         Quercus_Afares  0.023438  0.019531  0.031250  0.015625   \n",
       "\n",
       "      margin5   margin6   margin7  margin8  ...  texture55  texture56  \\\n",
       "0    0.011719  0.009766  0.027344      0.0  ...   0.007812   0.000000   \n",
       "1    0.025391  0.001953  0.019531      0.0  ...   0.000977   0.000000   \n",
       "2    0.003906  0.005859  0.068359      0.0  ...   0.154300   0.000000   \n",
       "3    0.021484  0.019531  0.023438      0.0  ...   0.000000   0.000977   \n",
       "4    0.013672  0.015625  0.005859      0.0  ...   0.096680   0.000000   \n",
       "..        ...       ...       ...      ...  ...        ...        ...   \n",
       "985  0.000000  0.148440  0.017578      0.0  ...   0.242190   0.000000   \n",
       "986  0.001953  0.000000  0.000000      0.0  ...   0.170900   0.000000   \n",
       "987  0.078125  0.003906  0.007812      0.0  ...   0.004883   0.000977   \n",
       "988  0.009766  0.000000  0.000000      0.0  ...   0.083008   0.030273   \n",
       "989  0.005859  0.019531  0.035156      0.0  ...   0.000000   0.000000   \n",
       "\n",
       "     texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0     0.002930   0.002930   0.035156   0.000000   0.000000   0.004883   \n",
       "1     0.000000   0.000977   0.023438   0.000000   0.000000   0.000977   \n",
       "2     0.005859   0.000977   0.007812   0.000000   0.000000   0.000000   \n",
       "3     0.000000   0.000000   0.020508   0.000000   0.000000   0.017578   \n",
       "4     0.021484   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "985   0.034180   0.000000   0.010742   0.000000   0.000000   0.000000   \n",
       "986   0.018555   0.000000   0.011719   0.000000   0.000000   0.000977   \n",
       "987   0.004883   0.027344   0.016602   0.007812   0.000000   0.027344   \n",
       "988   0.000977   0.002930   0.014648   0.000000   0.041992   0.000000   \n",
       "989   0.002930   0.000000   0.012695   0.000000   0.000000   0.023438   \n",
       "\n",
       "     texture63  texture64  \n",
       "0     0.000000   0.025391  \n",
       "1     0.039062   0.022461  \n",
       "2     0.020508   0.002930  \n",
       "3     0.000000   0.047852  \n",
       "4     0.000000   0.031250  \n",
       "..         ...        ...  \n",
       "985   0.000000   0.018555  \n",
       "986   0.000000   0.021484  \n",
       "987   0.000000   0.001953  \n",
       "988   0.001953   0.002930  \n",
       "989   0.025391   0.022461  \n",
       "\n",
       "[990 rows x 194 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "path = 'train.csv'\n",
    "df = read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1575</td>\n",
       "      <td>Magnolia_Salicifolia</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1578</td>\n",
       "      <td>Acer_Pictum</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1581</td>\n",
       "      <td>Alnus_Maximowiczii</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>1582</td>\n",
       "      <td>Quercus_Rubra</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1584</td>\n",
       "      <td>Quercus_Afares</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                species   margin1   margin2   margin3   margin4  \\\n",
       "0       1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1       2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2       3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3       5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4       6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "..    ...                    ...       ...       ...       ...       ...   \n",
       "985  1575   Magnolia_Salicifolia  0.060547  0.119140  0.007812  0.003906   \n",
       "986  1578            Acer_Pictum  0.001953  0.003906  0.021484  0.107420   \n",
       "987  1581     Alnus_Maximowiczii  0.001953  0.003906  0.000000  0.021484   \n",
       "988  1582          Quercus_Rubra  0.000000  0.000000  0.046875  0.056641   \n",
       "989  1584         Quercus_Afares  0.023438  0.019531  0.031250  0.015625   \n",
       "\n",
       "      margin5   margin6   margin7  margin8  ...  texture55  texture56  \\\n",
       "0    0.011719  0.009766  0.027344      0.0  ...   0.007812   0.000000   \n",
       "1    0.025391  0.001953  0.019531      0.0  ...   0.000977   0.000000   \n",
       "2    0.003906  0.005859  0.068359      0.0  ...   0.154300   0.000000   \n",
       "3    0.021484  0.019531  0.023438      0.0  ...   0.000000   0.000977   \n",
       "4    0.013672  0.015625  0.005859      0.0  ...   0.096680   0.000000   \n",
       "..        ...       ...       ...      ...  ...        ...        ...   \n",
       "985  0.000000  0.148440  0.017578      0.0  ...   0.242190   0.000000   \n",
       "986  0.001953  0.000000  0.000000      0.0  ...   0.170900   0.000000   \n",
       "987  0.078125  0.003906  0.007812      0.0  ...   0.004883   0.000977   \n",
       "988  0.009766  0.000000  0.000000      0.0  ...   0.083008   0.030273   \n",
       "989  0.005859  0.019531  0.035156      0.0  ...   0.000000   0.000000   \n",
       "\n",
       "     texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0     0.002930   0.002930   0.035156   0.000000   0.000000   0.004883   \n",
       "1     0.000000   0.000977   0.023438   0.000000   0.000000   0.000977   \n",
       "2     0.005859   0.000977   0.007812   0.000000   0.000000   0.000000   \n",
       "3     0.000000   0.000000   0.020508   0.000000   0.000000   0.017578   \n",
       "4     0.021484   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "985   0.034180   0.000000   0.010742   0.000000   0.000000   0.000000   \n",
       "986   0.018555   0.000000   0.011719   0.000000   0.000000   0.000977   \n",
       "987   0.004883   0.027344   0.016602   0.007812   0.000000   0.027344   \n",
       "988   0.000977   0.002930   0.014648   0.000000   0.041992   0.000000   \n",
       "989   0.002930   0.000000   0.012695   0.000000   0.000000   0.023438   \n",
       "\n",
       "     texture63  texture64  \n",
       "0     0.000000   0.025391  \n",
       "1     0.039062   0.022461  \n",
       "2     0.020508   0.002930  \n",
       "3     0.000000   0.047852  \n",
       "4     0.000000   0.031250  \n",
       "..         ...        ...  \n",
       "985   0.000000   0.018555  \n",
       "986   0.000000   0.021484  \n",
       "987   0.000000   0.001953  \n",
       "988   0.001953   0.002930  \n",
       "989   0.025391   0.022461  \n",
       "\n",
       "[990 rows x 194 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty cells\n",
    "df.dropna(inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1575</td>\n",
       "      <td>Magnolia_Salicifolia</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1578</td>\n",
       "      <td>Acer_Pictum</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1581</td>\n",
       "      <td>Alnus_Maximowiczii</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>1582</td>\n",
       "      <td>Quercus_Rubra</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1584</td>\n",
       "      <td>Quercus_Afares</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                species   margin1   margin2   margin3   margin4  \\\n",
       "0       1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1       2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2       3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3       5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4       6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "..    ...                    ...       ...       ...       ...       ...   \n",
       "985  1575   Magnolia_Salicifolia  0.060547  0.119140  0.007812  0.003906   \n",
       "986  1578            Acer_Pictum  0.001953  0.003906  0.021484  0.107420   \n",
       "987  1581     Alnus_Maximowiczii  0.001953  0.003906  0.000000  0.021484   \n",
       "988  1582          Quercus_Rubra  0.000000  0.000000  0.046875  0.056641   \n",
       "989  1584         Quercus_Afares  0.023438  0.019531  0.031250  0.015625   \n",
       "\n",
       "      margin5   margin6   margin7  margin8  ...  texture55  texture56  \\\n",
       "0    0.011719  0.009766  0.027344      0.0  ...   0.007812   0.000000   \n",
       "1    0.025391  0.001953  0.019531      0.0  ...   0.000977   0.000000   \n",
       "2    0.003906  0.005859  0.068359      0.0  ...   0.154300   0.000000   \n",
       "3    0.021484  0.019531  0.023438      0.0  ...   0.000000   0.000977   \n",
       "4    0.013672  0.015625  0.005859      0.0  ...   0.096680   0.000000   \n",
       "..        ...       ...       ...      ...  ...        ...        ...   \n",
       "985  0.000000  0.148440  0.017578      0.0  ...   0.242190   0.000000   \n",
       "986  0.001953  0.000000  0.000000      0.0  ...   0.170900   0.000000   \n",
       "987  0.078125  0.003906  0.007812      0.0  ...   0.004883   0.000977   \n",
       "988  0.009766  0.000000  0.000000      0.0  ...   0.083008   0.030273   \n",
       "989  0.005859  0.019531  0.035156      0.0  ...   0.000000   0.000000   \n",
       "\n",
       "     texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0     0.002930   0.002930   0.035156   0.000000   0.000000   0.004883   \n",
       "1     0.000000   0.000977   0.023438   0.000000   0.000000   0.000977   \n",
       "2     0.005859   0.000977   0.007812   0.000000   0.000000   0.000000   \n",
       "3     0.000000   0.000000   0.020508   0.000000   0.000000   0.017578   \n",
       "4     0.021484   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "985   0.034180   0.000000   0.010742   0.000000   0.000000   0.000000   \n",
       "986   0.018555   0.000000   0.011719   0.000000   0.000000   0.000977   \n",
       "987   0.004883   0.027344   0.016602   0.007812   0.000000   0.027344   \n",
       "988   0.000977   0.002930   0.014648   0.000000   0.041992   0.000000   \n",
       "989   0.002930   0.000000   0.012695   0.000000   0.000000   0.023438   \n",
       "\n",
       "     texture63  texture64  \n",
       "0     0.000000   0.025391  \n",
       "1     0.039062   0.022461  \n",
       "2     0.020508   0.002930  \n",
       "3     0.000000   0.047852  \n",
       "4     0.000000   0.031250  \n",
       "..         ...        ...  \n",
       "985   0.000000   0.018555  \n",
       "986   0.000000   0.021484  \n",
       "987   0.000000   0.001953  \n",
       "988   0.001953   0.002930  \n",
       "989   0.025391   0.022461  \n",
       "\n",
       "[990 rows x 194 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates\n",
    "df.drop_duplicates(inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "[0.005859 0.0 0.03125 0.015625 0.025391 0.001953 0.019531 0.0 0.0 0.007812\n",
      " 0.003906 0.027344 0.023438 0.0 0.033203 0.0 0.009766 0.009766 0.007812\n",
      " 0.007812 0.019531 0.007812 0.0 0.0 0.007812 0.027344 0.003906 0.037109\n",
      " 0.007812 0.048828 0.054688 0.027344 0.003906 0.0 0.0 0.003906 0.013672\n",
      " 0.033203 0.033203 0.019531 0.03125 0.009766 0.007812 0.03125 0.001953\n",
      " 0.039062 0.029297 0.03125 0.035156 0.0 0.007812 0.0 0.046875 0.046875\n",
      " 0.029297 0.009766 0.017578 0.007812 0.013672 0.019531 0.0 0.0 0.003906\n",
      " 0.0 0.00074942 0.00069461 0.0007198 0.00070948 0.00068849 0.00066046\n",
      " 0.00062381 0.00058531 0.00055583 0.00053091 0.00050553 0.00048417\n",
      " 0.00046714 0.00045311 0.0004424 0.0004347 0.00043541 0.00043311\n",
      " 0.00044305 0.00046024 0.00047142 0.00048563 0.00051198 0.00053842\n",
      " 0.000567 0.00060459 0.00064376 0.00068698 0.00073141 0.00077748\n",
      " 0.00083219 0.00088429 0.00090166 0.00086166 0.00084172 0.0007829\n",
      " 0.00073129 0.00067913 0.00063886 0.00059633 0.00056019 0.00052754\n",
      " 0.00049597 0.00047412 0.00046162 0.00044696 0.00043426 0.00043348\n",
      " 0.00043533 0.00043631 0.00044743 0.00046355 0.00047893 0.0004963\n",
      " 0.00051774 0.00054563 0.00057398 0.00060783 0.00064104 0.00067412\n",
      " 0.00070265 0.00070739 0.0006884 0.00074675 0.0 0.0 0.007812 0.079102 0.0\n",
      " 0.039062 0.000977 0.0 0.027344 0.003906 0.0 0.0 0.014648 0.041016 0.0 0.0\n",
      " 0.003906 0.0 0.020508 0.006836 0.0 0.001953 0.026367 0.020508 0.050781\n",
      " 0.001953 0.021484 0.003906 0.027344 0.023438 0.0625 0.0 0.038086 0.0\n",
      " 0.019531 0.0 0.001953 0.003906 0.015625 0.004883 0.10449 0.0 0.061523\n",
      " 0.007812 0.008789 0.013672 0.011719 0.001953 0.035156 0.007812 0.0 0.0\n",
      " 0.053711 0.036133 0.000977 0.0 0.0 0.000977 0.023438 0.0 0.0 0.000977\n",
      " 0.039062 0.022461]\n",
      "990\n"
     ]
    }
   ],
   "source": [
    "print(len(df['species'].unique()))\n",
    "print(df.values[:, 2:][1])\n",
    "print(len(df['species'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data description\n",
    "The dataset consists approximately 990 leaf specimens/images, and its features. Three sets of features are also provided per row or image: a shape contiguous descriptor, an interior texture histogram, and a ﬁne-scale margin histogram. For each feature, a 64-attribute vector is given per leaf sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# read image \n",
    "image = cv2.imread('images/1.jpg')\n",
    "# show the image, provide window name first\n",
    "cv2.imshow('image window 1', image)\n",
    "# add wait key. window waits until user presses a key\n",
    "cv2.waitKey(0)\n",
    "# and finally destroy/close all open windows\n",
    "cv2.destroyAllWindows()\n",
    "# read image \n",
    "image = cv2.imread('images/2.jpg')\n",
    "# show the image, provide window name first\n",
    "cv2.imshow('image window 2', image)\n",
    "# add wait key. window waits until user presses a key\n",
    "cv2.waitKey(0)\n",
    "# and finally destroy/close all open windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot-encoding and spit data into training set, validation set, and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(990, 192)\n",
      "(990,)\n",
      "[0.005859 0.0 0.03125 0.015625 0.025391 0.001953 0.019531 0.0 0.0 0.007812\n",
      " 0.003906 0.027344 0.023438 0.0 0.033203 0.0 0.009766 0.009766 0.007812\n",
      " 0.007812 0.019531 0.007812 0.0 0.0 0.007812 0.027344 0.003906 0.037109\n",
      " 0.007812 0.048828 0.054688 0.027344 0.003906 0.0 0.0 0.003906 0.013672\n",
      " 0.033203 0.033203 0.019531 0.03125 0.009766 0.007812 0.03125 0.001953\n",
      " 0.039062 0.029297 0.03125 0.035156 0.0 0.007812 0.0 0.046875 0.046875\n",
      " 0.029297 0.009766 0.017578 0.007812 0.013672 0.019531 0.0 0.0 0.003906\n",
      " 0.0 0.00074942 0.00069461 0.0007198 0.00070948 0.00068849 0.00066046\n",
      " 0.00062381 0.00058531 0.00055583 0.00053091 0.00050553 0.00048417\n",
      " 0.00046714 0.00045311 0.0004424 0.0004347 0.00043541 0.00043311\n",
      " 0.00044305 0.00046024 0.00047142 0.00048563 0.00051198 0.00053842\n",
      " 0.000567 0.00060459 0.00064376 0.00068698 0.00073141 0.00077748\n",
      " 0.00083219 0.00088429 0.00090166 0.00086166 0.00084172 0.0007829\n",
      " 0.00073129 0.00067913 0.00063886 0.00059633 0.00056019 0.00052754\n",
      " 0.00049597 0.00047412 0.00046162 0.00044696 0.00043426 0.00043348\n",
      " 0.00043533 0.00043631 0.00044743 0.00046355 0.00047893 0.0004963\n",
      " 0.00051774 0.00054563 0.00057398 0.00060783 0.00064104 0.00067412\n",
      " 0.00070265 0.00070739 0.0006884 0.00074675 0.0 0.0 0.007812 0.079102 0.0\n",
      " 0.039062 0.000977 0.0 0.027344 0.003906 0.0 0.0 0.014648 0.041016 0.0 0.0\n",
      " 0.003906 0.0 0.020508 0.006836 0.0 0.001953 0.026367 0.020508 0.050781\n",
      " 0.001953 0.021484 0.003906 0.027344 0.023438 0.0625 0.0 0.038086 0.0\n",
      " 0.019531 0.0 0.001953 0.003906 0.015625 0.004883 0.10449 0.0 0.061523\n",
      " 0.007812 0.008789 0.013672 0.011719 0.001953 0.035156 0.007812 0.0 0.0\n",
      " 0.053711 0.036133 0.000977 0.0 0.0 0.000977 0.023438 0.0 0.0 0.000977\n",
      " 0.039062 0.022461]\n",
      "Pterocarya_Stenoptera\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# split into input and output columns\n",
    "X, y = df.values[:, 2:], df['species'].values\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X[1])\n",
    "print(y[1])\n",
    "X = X.astype('float64') # ensure all data are floating point values\n",
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "print(y[1])\n",
    "# y_one_hot_encoding = LabelBinarizer().fit_transform(y)\n",
    "# print(y_one_hot_encoding.shape)\n",
    "# print(y_one_hot_encoding[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 192) (198, 192) (594,) (198,) (198, 192) (198,)\n",
      "(594, 192) (198, 192) (594,) (198,) (198, 192) (198,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train validation, and test datasets\n",
    "#\n",
    "xrest, X_test, yrest, y_test = train_test_split(X, y, test_size=0.2,train_size=0.8 , shuffle = True , random_state=42)\n",
    "X_train, xval, y_train, yval = train_test_split(xrest,yrest,test_size = 0.25,train_size =0.75 , shuffle = True , random_state=42)\n",
    "\n",
    "#\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, xval.shape, yval.shape)\n",
    "\n",
    "# normalization\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler() # we estimated the mean and varience of the training set, and divide by train set to make mean = 0 ; standard deviation = 1.0\n",
    "X_train = scaler.fit_transform(X_train) # we use the same estimation to test set\n",
    "X_test = scaler.transform(X_test)\n",
    "xval = scaler.transform(xval)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, xval.shape, yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 99\n"
     ]
    }
   ],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1] # X_train.shape[0] is no. of training samples, and X_train.shape[1] is no. of features in training sample\n",
    "n_classes = len(np.unique(df['species'].values))\n",
    "print(n_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 192) (198, 192) (594, 99) (198, 99) (198, 192) (198, 99)\n"
     ]
    }
   ],
   "source": [
    "# one-hot-encoding\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "y_train=to_categorical(y_train,n_classes)\n",
    "y_test=to_categorical(y_test,n_classes)\n",
    "yval=to_categorical(yval,n_classes)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, xval.shape, yval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id   margin1   margin2   margin3   margin4   margin5  \\\n",
      "id         1.000000 -0.011673 -0.027565 -0.059533  0.001639 -0.002419   \n",
      "margin1   -0.011673  1.000000  0.806390 -0.182829 -0.297807 -0.475874   \n",
      "margin2   -0.027565  0.806390  1.000000 -0.204640 -0.315953 -0.444312   \n",
      "margin3   -0.059533 -0.182829 -0.204640  1.000000  0.120042 -0.185007   \n",
      "margin4    0.001639 -0.297807 -0.315953  0.120042  1.000000  0.029480   \n",
      "...             ...       ...       ...       ...       ...       ...   \n",
      "texture60 -0.000823  0.035072  0.081069 -0.019850 -0.052317  0.006542   \n",
      "texture61  0.026319 -0.007581 -0.007057  0.084957  0.320644 -0.109229   \n",
      "texture62  0.032873 -0.033159 -0.037405 -0.081999 -0.073886  0.151675   \n",
      "texture63  0.024299 -0.075171 -0.098957 -0.148193  0.050970  0.022299   \n",
      "texture64  0.035396  0.030414 -0.029532  0.061780  0.014343 -0.148834   \n",
      "\n",
      "            margin6   margin7   margin8   margin9  ...  texture55  texture56  \\\n",
      "id        -0.051818  0.061214 -0.039509 -0.070954  ...  -0.040292  -0.005132   \n",
      "margin1    0.767718  0.066273 -0.094137 -0.181496  ...   0.137158  -0.047771   \n",
      "margin2    0.825762 -0.083273 -0.086428 -0.120276  ...   0.154407  -0.021096   \n",
      "margin3   -0.163976  0.095449  0.024350 -0.000042  ...   0.047347  -0.027618   \n",
      "margin4   -0.261437 -0.268271 -0.047693  0.227543  ...  -0.071974  -0.009537   \n",
      "...             ...       ...       ...       ...  ...        ...        ...   \n",
      "texture60  0.066262 -0.034094  0.048647 -0.028292  ...  -0.129365   0.004412   \n",
      "texture61 -0.050498 -0.163375 -0.079283  0.088517  ...  -0.002235   0.053707   \n",
      "texture62 -0.031555  0.015391 -0.048843 -0.031954  ...  -0.217239   0.171577   \n",
      "texture63 -0.132087 -0.001364  0.027758 -0.119494  ...  -0.207887   0.002057   \n",
      "texture64 -0.003164  0.068512 -0.003191 -0.097760  ...  -0.095205  -0.095913   \n",
      "\n",
      "           texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
      "id         -0.043101   0.063337  -0.007915  -0.000823   0.026319   0.032873   \n",
      "margin1     0.126227  -0.024139  -0.168201   0.035072  -0.007581  -0.033159   \n",
      "margin2     0.123834  -0.063654  -0.157842   0.081069  -0.007057  -0.037405   \n",
      "margin3     0.007261  -0.021390   0.033505  -0.019850   0.084957  -0.081999   \n",
      "margin4    -0.050529  -0.044318   0.088857  -0.052317   0.320644  -0.073886   \n",
      "...              ...        ...        ...        ...        ...        ...   \n",
      "texture60  -0.155187   0.240704  -0.183369   1.000000  -0.051838   0.265879   \n",
      "texture61  -0.072814  -0.084638  -0.023539  -0.051838   1.000000  -0.063582   \n",
      "texture62  -0.283316   0.563088  -0.128010   0.265879  -0.063582   1.000000   \n",
      "texture63  -0.064724  -0.059866   0.156568  -0.089679  -0.068065  -0.058189   \n",
      "texture64   0.224686  -0.269157  -0.015374  -0.190194   0.036374  -0.245527   \n",
      "\n",
      "           texture63  texture64  \n",
      "id          0.024299   0.035396  \n",
      "margin1    -0.075171   0.030414  \n",
      "margin2    -0.098957  -0.029532  \n",
      "margin3    -0.148193   0.061780  \n",
      "margin4     0.050970   0.014343  \n",
      "...              ...        ...  \n",
      "texture60  -0.089679  -0.190194  \n",
      "texture61  -0.068065   0.036374  \n",
      "texture62  -0.058189  -0.245527  \n",
      "texture63   1.000000   0.029305  \n",
      "texture64   0.029305   1.000000  \n",
      "\n",
      "[193 rows x 193 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**no much correlation between certain column and the other columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((594, 99), (594, 192))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 2s 5ms/step - loss: 4.5362 - categorical_accuracy: 0.0505 \n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 3.1434 - categorical_accuracy: 0.4343\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 2.2554 - categorical_accuracy: 0.6919\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.6425 - categorical_accuracy: 0.7980\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 1.1954 - categorical_accuracy: 0.8838\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.8973 - categorical_accuracy: 0.9242\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6754 - categorical_accuracy: 0.9697\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5191 - categorical_accuracy: 0.9949\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4021 - categorical_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3143 - categorical_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2517 - categorical_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.2073 - categorical_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1745 - categorical_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.1485 - categorical_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1295 - categorical_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1136 - categorical_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.1009 - categorical_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0902 - categorical_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0815 - categorical_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0741 - categorical_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0680 - categorical_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0626 - categorical_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0578 - categorical_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0536 - categorical_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0497 - categorical_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0465 - categorical_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0437 - categorical_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0411 - categorical_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0387 - categorical_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0365 - categorical_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0345 - categorical_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0329 - categorical_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0312 - categorical_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0296 - categorical_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0283 - categorical_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0270 - categorical_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0257 - categorical_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0246 - categorical_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0236 - categorical_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0227 - categorical_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0218 - categorical_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0210 - categorical_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0201 - categorical_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0194 - categorical_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0187 - categorical_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0180 - categorical_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0174 - categorical_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.0168 - categorical_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0163 - categorical_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.0158 - categorical_accuracy: 1.0000\n",
      "Best: 0.575758 using {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.046836) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.043446) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.044605) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.037113) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.063484) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.560606 (0.049485) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.049485) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.025753) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.046836) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.043446) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.046836) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.037113) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.035712) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.049997) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.037795) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.055785) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.046836) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.046836) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.053925) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.063484) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.049997) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.062267) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.575758 (0.037113) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.032731) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.025753) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.068135) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.043446) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.012371) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.049997) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.061025) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.039768) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.049997) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.031133) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.049997) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.063484) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.055785) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.063484) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.051505) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.051505) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.032731) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.560606 (0.037113) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.049485) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.044605) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.049997) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.025753) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.037113) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.058464) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.570707 (0.051505) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.035712) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.037795) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.037795) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.031133) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.032731) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.049997) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.454545 (0.037113) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.025753) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.063484) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.044605) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.037795) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.042855) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.094486) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.474747 (0.039768) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.056692) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.032731) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.068135) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.043446) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.044605) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.037113) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.051505) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.031133) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.051505) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.065462) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.053925) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.035712) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.051505) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.039768) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.046836) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.064282) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.037113) with: {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.042855) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.056692) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.058464) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.046836) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.068135) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.039768) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.057140) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.068135) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.049485) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.031133) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.053925) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.063484) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.051505) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.032731) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.039768) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.032731) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.051505) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.062267) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.049997) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.565657 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.043446) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.565657 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.063484) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.058464) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.053925) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.053925) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.061025) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.046836) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.075251) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.044605) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.039768) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.049997) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.068135) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.037795) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.075251) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.055785) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.043446) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.058464) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.053925) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.560606 (0.049485) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.055785) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.560606 (0.037113) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.046836) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.565657 (0.043446) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.039768) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.039768) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.051505) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.049485) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.075589) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.037113) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.043446) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.051505) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.053925) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.057140) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.061025) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.057140) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535354 (0.049997) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.061025) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.061025) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.070345) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.044605) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.031133) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.018897) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.043446) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.064282) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.049485) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.044605) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.031133) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.053925) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.058464) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.046836) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.056692) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.035712) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.035712) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.056692) with: {'batch_size': 32, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.031133) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.061025) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.068135) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.042855) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.055785) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.469697 (0.065462) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.032731) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.049997) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.474747 (0.028570) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.046836) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.043446) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.043446) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.042855) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.028570) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.062267) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.039768) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.035712) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.018897) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.039768) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.061025) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.032731) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.037795) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.021427) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.565657 (0.049997) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.043446) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.037113) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.570707 (0.051505) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.434343 (0.035712) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.042855) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.049485) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.469697 (0.032731) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.058464) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.042855) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.043446) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.037795) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.058464) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.063484) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.061856) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.044605) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.044605) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.032731) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.025753) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.055785) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.049485) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.032731) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.018897) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.055785) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.028570) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.068135) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.044605) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.044605) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.032731) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.025753) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.419192 (0.046836) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.414141 (0.007142) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.454545 (0.049485) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.393939 (0.044605) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.409091 (0.056692) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.409091 (0.037113) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.444444 (0.028570) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.393939 (0.049485) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.393939 (0.044605) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.037795) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.469697 (0.049485) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.061025) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.049997) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.039768) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.071425) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.464646 (0.043446) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.063484) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.464646 (0.018897) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.039768) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.057140) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.044605) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.051505) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.058464) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.068880) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.053925) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.044605) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.056692) with: {'batch_size': 64, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.028570) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.064282) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.025753) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.044605) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.063484) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.044605) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.065462) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.025753) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.056692) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.042855) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.025753) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.061025) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.072488) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.058464) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.057140) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.061025) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.039768) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.051505) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.565657 (0.055785) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.055785) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.039768) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.037795) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.560606 (0.024742) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.042855) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.049997) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.061025) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.053925) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.072488) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.093400) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.053925) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.039768) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.025753) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.025753) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.051505) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.049997) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.039768) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.032731) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.028570) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.051505) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.032731) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.042855) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.049997) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.037113) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.024742) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.065462) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.049485) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.057140) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.570707 (0.039768) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.049997) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.021427) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.051505) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.560606 (0.032731) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.037795) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.049997) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.053925) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.055785) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.042855) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.031133) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.068880) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.055785) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.056692) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.085710) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.018897) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.064282) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.049997) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.037795) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.031133) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.043446) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.062267) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.042855) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.062267) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.031133) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.049997) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.570707 (0.028570) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.031133) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.061025) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.051505) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.049997) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.024742) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.037795) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.072488) with: {'batch_size': 64, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.429293 (0.051505) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.434343 (0.031133) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.434343 (0.037795) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.449495 (0.039768) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.454545 (0.049485) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.464646 (0.025753) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.072488) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.434343 (0.046836) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.419192 (0.007142) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.049997) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.058464) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.043446) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.032731) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.058464) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.025753) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.055785) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.061025) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.018897) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.046836) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.053925) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.051505) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.049997) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.028570) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.037795) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.032731) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.031133) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.042855) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.429293 (0.072488) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.388889 (0.079535) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.404040 (0.046836) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.414141 (0.049997) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.414141 (0.055785) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.363636 (0.012371) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.383838 (0.049997) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.414141 (0.055785) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.388889 (0.007142) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.474747 (0.031133) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.439394 (0.037113) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.454545 (0.074227) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.063484) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.049997) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.046836) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.039768) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.469697 (0.032731) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.061025) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.021427) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.012371) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.049997) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.042855) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.056692) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.075251) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.053925) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.018897) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.077258) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.338384 (0.037795) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.368687 (0.007142) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.353535 (0.043446) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.373737 (0.039768) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.333333 (0.012371) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.358586 (0.031133) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.348485 (0.012371) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.303030 (0.000000) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.343434 (0.037795) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.363636 (0.037113) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.398990 (0.028570) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.409091 (0.065462) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.409091 (0.044605) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.419192 (0.031133) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.414141 (0.039768) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.378788 (0.012371) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.419192 (0.063484) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.363636 (0.042855) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.065462) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.464646 (0.068135) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.449495 (0.035712) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.043446) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.454545 (0.044605) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.469697 (0.042855) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.444444 (0.037795) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.454545 (0.065462) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.439394 (0.044605) with: {'batch_size': 128, 'epochs': 50, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.024742) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.058464) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.071425) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.046836) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.049997) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.510101 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.056692) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.053925) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.018897) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.063484) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.042855) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.025753) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.565657 (0.035712) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.065462) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.039768) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.044605) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.064282) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.055785) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.032731) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.028570) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.555556 (0.018897) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.028570) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.039768) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.018897) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.056692) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.444444 (0.051505) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.500000 (0.049485) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.068135) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.070345) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.464646 (0.051505) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.028570) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.049997) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.032731) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.071425) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.044605) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.061856) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.505051 (0.051505) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.540404 (0.043446) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.046836) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.068135) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.545455 (0.032731) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.550505 (0.028570) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.046836) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.058464) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.039768) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.024742) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.031133) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.535354 (0.051505) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.4, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.454545 (0.065462) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.444444 (0.051505) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.419192 (0.046836) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.469697 (0.037113) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.439394 (0.021427) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.414141 (0.061025) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.444444 (0.051505) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.459596 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.479798 (0.055785) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 128, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.049485) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.484848 (0.044605) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.031133) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.018897) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.039768) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.494949 (0.043446) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.039768) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.489899 (0.075589) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 200, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.053925) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.046836) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.061025) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'SGD', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.520202 (0.043446) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.515152 (0.068880) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.046836) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n",
      "0.530303 (0.044605) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.037795) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.525253 (0.055785) with: {'batch_size': 128, 'epochs': 100, 'model__dropout_rate': 0.7, 'model__neurons': 300, 'optimizer': 'Adam', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "from keras.layers import Dropout\n",
    "def create_model(neurons, dropout_rate, optimizer='adam'):\n",
    "    # create model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='tanh', input_shape=(n_features,)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,  metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(model=create_model, verbose=1)\n",
    "# define the grid search parameters\n",
    "batch_size = [32, 64, 128]\n",
    "epochs = [50, 100]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adam']\n",
    "learn_rate = [0.001, 0.01, 0.0001]\n",
    "momentum = [0.0]\n",
    "dropout_rate = [0.0, 0.4, 0.7]\n",
    "neurons = [128, 200, 300]\n",
    "param_grid = dict(optimizer__learning_rate=learn_rate, optimizer__momentum=momentum, optimizer=optimizer, batch_size=batch_size, epochs=epochs, model__dropout_rate=dropout_rate, model__neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(xval, yval, verbose = 1)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.575758 using {'batch_size': 32, 'epochs': 50, 'model__dropout_rate': 0.0, 'model__neurons': 300, 'optimizer': 'RMSprop', 'optimizer__learning_rate': 0.0001, 'optimizer__momentum': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# best parameters\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.0 RMSprop 50 32 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "parameters = grid_result.best_params_\n",
    "nuerons = parameters['model__neurons']\n",
    "dropout_rate = parameters['model__dropout_rate']\n",
    "optimizer = parameters['optimizer']\n",
    "epochs = parameters['epochs']\n",
    "batch_size = parameters['batch_size']\n",
    "print(nuerons, dropout_rate, optimizer, int(epochs), batch_size, type(int(epochs)))\n",
    "model = Sequential()\n",
    "model.add(Dense(int(nuerons), activation='tanh', input_shape=(n_features,)))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 4ms/step - loss: 3.5295 - categorical_accuracy: 0.3064\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1.7149 - categorical_accuracy: 0.8064\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.9551 - categorical_accuracy: 0.9276\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5406 - categorical_accuracy: 0.9680\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3244 - categorical_accuracy: 0.9832\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1955 - categorical_accuracy: 0.9983\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.1168 - categorical_accuracy: 0.9983\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0795 - categorical_accuracy: 0.9983\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0481 - categorical_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0315 - categorical_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0225 - categorical_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0149 - categorical_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0107 - categorical_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0082 - categorical_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0064 - categorical_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0053 - categorical_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0044 - categorical_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0037 - categorical_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0033 - categorical_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0029 - categorical_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0026 - categorical_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0024 - categorical_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0022 - categorical_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0020 - categorical_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0019 - categorical_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0017 - categorical_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0016 - categorical_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0015 - categorical_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0014 - categorical_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0014 - categorical_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0013 - categorical_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0012 - categorical_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0011 - categorical_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0011 - categorical_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.0010 - categorical_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 9.9970e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 9.6125e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 9.2665e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 8.9486e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 8.6549e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 8.3662e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 8.1076e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 7.8542e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 7.6317e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 7.4188e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 7.2036e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 7.0110e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.8295e-04 - categorical_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.6469e-04 - categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a7d7cfbd88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,  metrics=['categorical_accuracy'])\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 3ms/step - loss: 0.2049 - categorical_accuracy: 0.9697\n",
      "Test Accuracy: 0.970\n",
      "loss 0.20494960248470306\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "    return loss, acc\n",
    "loss, acc = evaluate_model(model, X_test, y_test)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1579</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.006836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1580</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136720</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   margin1   margin2   margin3   margin4   margin5   margin6  \\\n",
       "0       4  0.019531  0.009766  0.078125  0.011719  0.003906  0.015625   \n",
       "1       7  0.007812  0.005859  0.064453  0.009766  0.003906  0.013672   \n",
       "2       9  0.000000  0.000000  0.001953  0.021484  0.041016  0.000000   \n",
       "3      12  0.000000  0.000000  0.009766  0.011719  0.017578  0.000000   \n",
       "4      13  0.001953  0.000000  0.015625  0.009766  0.039062  0.000000   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "589  1576  0.000000  0.000000  0.003906  0.015625  0.041016  0.000000   \n",
       "590  1577  0.000000  0.003906  0.003906  0.005859  0.017578  0.000000   \n",
       "591  1579  0.017578  0.029297  0.015625  0.013672  0.003906  0.015625   \n",
       "592  1580  0.013672  0.009766  0.060547  0.025391  0.035156  0.025391   \n",
       "593  1583  0.000000  0.117190  0.000000  0.019531  0.000000  0.136720   \n",
       "\n",
       "      margin7   margin8   margin9  ...  texture55  texture56  texture57  \\\n",
       "0    0.005859  0.000000  0.005859  ...   0.006836   0.000000   0.015625   \n",
       "1    0.007812  0.000000  0.033203  ...   0.000000   0.000000   0.006836   \n",
       "2    0.023438  0.000000  0.011719  ...   0.128910   0.000000   0.000977   \n",
       "3    0.003906  0.000000  0.003906  ...   0.012695   0.015625   0.002930   \n",
       "4    0.009766  0.000000  0.005859  ...   0.000000   0.042969   0.016602   \n",
       "..        ...       ...       ...  ...        ...        ...        ...   \n",
       "589  0.017578  0.000000  0.005859  ...   0.098633   0.000000   0.004883   \n",
       "590  0.017578  0.005859  0.000000  ...   0.012695   0.004883   0.004883   \n",
       "591  0.025391  0.000000  0.000000  ...   0.073242   0.000000   0.028320   \n",
       "592  0.039062  0.000000  0.003906  ...   0.003906   0.000000   0.000977   \n",
       "593  0.001953  0.005859  0.000000  ...   0.107420   0.012695   0.016602   \n",
       "\n",
       "     texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "0     0.000977   0.015625        0.0        0.0   0.000000   0.003906   \n",
       "1     0.001953   0.013672        0.0        0.0   0.000977   0.037109   \n",
       "2     0.000000   0.000000        0.0        0.0   0.015625   0.000000   \n",
       "3     0.036133   0.013672        0.0        0.0   0.089844   0.000000   \n",
       "4     0.010742   0.041016        0.0        0.0   0.007812   0.009766   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "589   0.000000   0.003906        0.0        0.0   0.018555   0.000000   \n",
       "590   0.002930   0.009766        0.0        0.0   0.090820   0.000000   \n",
       "591   0.000000   0.001953        0.0        0.0   0.000000   0.042969   \n",
       "592   0.000000   0.011719        0.0        0.0   0.000000   0.011719   \n",
       "593   0.000977   0.004883        0.0        0.0   0.015625   0.000000   \n",
       "\n",
       "     texture64  \n",
       "0     0.053711  \n",
       "1     0.044922  \n",
       "2     0.000000  \n",
       "3     0.008789  \n",
       "4     0.007812  \n",
       "..         ...  \n",
       "589   0.000977  \n",
       "590   0.016602  \n",
       "591   0.006836  \n",
       "592   0.018555  \n",
       "593   0.017578  \n",
       "\n",
       "[594 rows x 193 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "path = 'test.csv'\n",
    "test_df = read_csv(path)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(594, 193)\n",
      "0.019531\n",
      "(594, 192)\n",
      "[ 1.03008102e-01 -4.89578381e-01  1.86357296e+00 -3.85267810e-01\n",
      " -5.49861318e-01 -4.52044328e-01 -7.90785284e-01 -4.19629064e-01\n",
      " -1.21437934e-01  2.56155413e-01 -7.05013499e-01  8.44298651e-01\n",
      "  7.21685644e-01 -4.30934077e-01  1.41477307e+00 -9.39750562e-02\n",
      " -1.25898538e+00  5.71652083e-02 -7.18400438e-01  2.31022229e-01\n",
      " -4.81319618e-01 -6.00640596e-01 -2.21545845e-01 -1.32325376e-01\n",
      " -7.01441819e-02 -2.55965256e-01 -4.94056343e-01  1.65648002e-01\n",
      "  5.88850723e-02  2.85993873e-01 -5.43559223e-01 -3.04492135e-01\n",
      "  3.70309809e-01 -5.17467736e-01  2.28076860e-02 -7.40273127e-01\n",
      "  2.35775311e+00  1.46253971e+00  1.26901711e+00 -1.87230134e-01\n",
      " -2.94465548e-01 -1.09072028e+00  1.32612886e-01  1.11962601e+00\n",
      " -7.67192204e-01 -6.39964479e-01 -8.08408393e-02  1.33838501e+00\n",
      " -5.39153280e-01 -1.31765075e-01  1.90848182e-01 -1.04451956e-01\n",
      " -6.97875307e-01 -3.79038738e-01 -4.91635584e-01 -6.72590648e-01\n",
      " -3.05715785e-01 -8.85164037e-01  7.45717689e-01 -9.56002885e-01\n",
      " -5.99831610e-01 -4.66368964e-01  1.38560196e+00 -4.57653360e-01\n",
      "  1.57982371e-01  4.15960203e-01  7.08554299e-01  1.01803742e+00\n",
      "  1.37957918e+00  1.73445920e+00  2.02770649e+00  2.33407405e+00\n",
      "  2.59908735e+00  2.84056830e+00  3.05637819e+00  3.27922321e+00\n",
      "  3.38549216e+00  3.01670612e+00  2.65960652e+00  2.35189716e+00\n",
      "  2.09806364e+00  1.91031450e+00  1.81656911e+00  1.69916424e+00\n",
      "  1.60060419e+00  1.46132104e+00  1.29928364e+00  1.12825256e+00\n",
      "  9.04294422e-01  7.07895198e-01  4.89432965e-01  2.52199293e-01\n",
      "  3.80225324e-02 -6.24201553e-02 -1.02977194e-01 -9.92296186e-02\n",
      " -7.19335094e-03  1.68321336e-01  4.45044275e-01  7.92947538e-01\n",
      "  1.15862156e+00  1.51195714e+00  1.87396530e+00  2.17480760e+00\n",
      "  2.41535824e+00  2.56281996e+00  2.75740677e+00  2.83030659e+00\n",
      "  2.78140056e+00  2.70759789e+00  2.59333528e+00  2.37254572e+00\n",
      "  2.13582255e+00  1.92567819e+00  1.77199657e+00  1.60765134e+00\n",
      "  1.51679642e+00  1.39340643e+00  1.22113506e+00  1.00907967e+00\n",
      "  7.75332758e-01  5.38604492e-01  3.90055863e-01  2.55635017e-01\n",
      "  1.27218977e-01  7.88581609e-02  5.06743769e-02  1.31080247e-02\n",
      "  1.87607146e+00  8.43136087e-02  1.01081048e-01 -4.68325088e-01\n",
      " -4.12831709e-01  5.60284666e-01  1.63063690e+00  3.73539221e-01\n",
      "  1.06600896e+00 -2.97328369e-01 -3.58899908e-01 -3.59091012e-01\n",
      " -7.23829589e-02 -2.21933132e-01 -1.92795730e-01 -3.89100056e-01\n",
      "  7.16901549e-02 -3.85384988e-01  1.06606666e-03  4.86229091e-01\n",
      " -2.68624270e-01 -5.67266881e-01  3.20684207e-01 -5.58981844e-01\n",
      " -3.41326519e-01 -5.40346356e-01 -2.22327406e-01 -4.57267809e-01\n",
      "  9.46102328e-01  9.65613411e-01  8.15971801e-01 -3.88040903e-01\n",
      " -3.86231275e-01  9.95729601e-01  1.02042472e-01 -2.43422473e-01\n",
      " -3.45600091e-01  1.08093207e+00  3.00926295e-01  2.71264629e-01\n",
      " -3.77648689e-01 -4.57317706e-01 -1.93058394e-01 -1.03118155e-01\n",
      " -1.32794448e-01  4.42435653e-01  3.99893250e-01 -5.71735079e-01\n",
      "  2.63311938e-01  2.44509927e-02 -4.09705479e-01 -5.16567159e-01\n",
      " -6.45579163e-01 -2.66847602e-01 -4.54119188e-01 -2.58105155e-01\n",
      " -3.79373232e-02 -4.04737488e-01 -3.57760564e-02 -2.38210885e-01\n",
      " -2.05724818e-01 -4.96437742e-01 -3.93676127e-01  1.46579776e+00]\n"
     ]
    }
   ],
   "source": [
    "# split into input and output columns\n",
    "import numpy as np\n",
    "test_X = test_df.values[:, 0:]\n",
    "print(test_X.shape)\n",
    "print(test_X[0][1])\n",
    "test_X = test_X.astype('float64') # ensure all data are floating point values\n",
    "x_test = test_X[:, 1:]\n",
    "x_test = scaler.transform(x_test)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "import pandas as pd  \n",
    "def eval_model(model, X_test, class_names, ids):\n",
    "    y_pred = model.predict(X_test)\n",
    "    k_df = pd.DataFrame(y_pred, columns=class_names)\n",
    "    k_df['id'] = ids\n",
    "    return k_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acer_Capillipes</th>\n",
       "      <th>Acer_Circinatum</th>\n",
       "      <th>Acer_Mono</th>\n",
       "      <th>Acer_Opalus</th>\n",
       "      <th>Acer_Palmatum</th>\n",
       "      <th>Acer_Pictum</th>\n",
       "      <th>Acer_Platanoids</th>\n",
       "      <th>Acer_Rubrum</th>\n",
       "      <th>Acer_Rufinerve</th>\n",
       "      <th>Acer_Saccharinum</th>\n",
       "      <th>...</th>\n",
       "      <th>Salix_Intergra</th>\n",
       "      <th>Sorbus_Aria</th>\n",
       "      <th>Tilia_Oliveri</th>\n",
       "      <th>Tilia_Platyphyllos</th>\n",
       "      <th>Tilia_Tomentosa</th>\n",
       "      <th>Ulmus_Bergmanniana</th>\n",
       "      <th>Viburnum_Tinus</th>\n",
       "      <th>Viburnum_x_Rhytidophylloides</th>\n",
       "      <th>Zelkova_Serrata</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.168964e-06</td>\n",
       "      <td>1.136802e-05</td>\n",
       "      <td>1.058141e-07</td>\n",
       "      <td>4.820798e-05</td>\n",
       "      <td>3.256654e-06</td>\n",
       "      <td>4.137841e-05</td>\n",
       "      <td>7.403700e-08</td>\n",
       "      <td>3.047323e-06</td>\n",
       "      <td>1.266115e-05</td>\n",
       "      <td>7.579653e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.759318e-04</td>\n",
       "      <td>4.958602e-06</td>\n",
       "      <td>3.458318e-07</td>\n",
       "      <td>1.498040e-05</td>\n",
       "      <td>1.033900e-05</td>\n",
       "      <td>4.679465e-07</td>\n",
       "      <td>1.301285e-07</td>\n",
       "      <td>3.894677e-04</td>\n",
       "      <td>1.840167e-07</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.404108e-07</td>\n",
       "      <td>3.778491e-08</td>\n",
       "      <td>3.627792e-07</td>\n",
       "      <td>4.197656e-06</td>\n",
       "      <td>1.302392e-07</td>\n",
       "      <td>2.827330e-07</td>\n",
       "      <td>1.338912e-05</td>\n",
       "      <td>5.901102e-07</td>\n",
       "      <td>4.014803e-07</td>\n",
       "      <td>5.012888e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.233165e-06</td>\n",
       "      <td>9.369525e-09</td>\n",
       "      <td>1.397477e-09</td>\n",
       "      <td>4.831081e-10</td>\n",
       "      <td>2.937486e-05</td>\n",
       "      <td>5.636082e-07</td>\n",
       "      <td>3.783148e-05</td>\n",
       "      <td>2.054628e-07</td>\n",
       "      <td>1.343156e-07</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.556525e-07</td>\n",
       "      <td>9.934249e-01</td>\n",
       "      <td>2.928195e-06</td>\n",
       "      <td>4.801346e-07</td>\n",
       "      <td>3.973553e-03</td>\n",
       "      <td>5.642667e-06</td>\n",
       "      <td>5.375386e-07</td>\n",
       "      <td>3.463326e-05</td>\n",
       "      <td>1.395726e-04</td>\n",
       "      <td>3.332193e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.319929e-07</td>\n",
       "      <td>4.067069e-07</td>\n",
       "      <td>9.261282e-07</td>\n",
       "      <td>1.506777e-07</td>\n",
       "      <td>7.147002e-07</td>\n",
       "      <td>2.642378e-06</td>\n",
       "      <td>1.349549e-08</td>\n",
       "      <td>2.454829e-07</td>\n",
       "      <td>2.621117e-04</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.353248e-06</td>\n",
       "      <td>1.768170e-02</td>\n",
       "      <td>5.160257e-06</td>\n",
       "      <td>3.126537e-06</td>\n",
       "      <td>1.798328e-05</td>\n",
       "      <td>1.564438e-06</td>\n",
       "      <td>1.352905e-04</td>\n",
       "      <td>1.044371e-04</td>\n",
       "      <td>2.254270e-04</td>\n",
       "      <td>2.053770e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.983198e-06</td>\n",
       "      <td>7.081412e-05</td>\n",
       "      <td>1.223778e-05</td>\n",
       "      <td>2.980604e-06</td>\n",
       "      <td>7.013277e-05</td>\n",
       "      <td>5.569041e-03</td>\n",
       "      <td>1.584182e-05</td>\n",
       "      <td>1.661617e-06</td>\n",
       "      <td>8.732156e-04</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.034397e-06</td>\n",
       "      <td>8.090228e-05</td>\n",
       "      <td>7.313113e-08</td>\n",
       "      <td>2.690308e-08</td>\n",
       "      <td>1.299203e-05</td>\n",
       "      <td>3.201723e-07</td>\n",
       "      <td>1.299397e-05</td>\n",
       "      <td>1.374264e-05</td>\n",
       "      <td>9.268543e-04</td>\n",
       "      <td>2.753391e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143235e-06</td>\n",
       "      <td>1.649982e-05</td>\n",
       "      <td>1.602802e-06</td>\n",
       "      <td>7.087561e-05</td>\n",
       "      <td>1.862322e-05</td>\n",
       "      <td>3.217035e-03</td>\n",
       "      <td>5.068738e-06</td>\n",
       "      <td>5.109041e-05</td>\n",
       "      <td>1.457895e-06</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>6.380034e-07</td>\n",
       "      <td>9.980335e-01</td>\n",
       "      <td>6.156577e-06</td>\n",
       "      <td>1.200267e-07</td>\n",
       "      <td>4.377605e-04</td>\n",
       "      <td>1.970486e-07</td>\n",
       "      <td>1.943734e-07</td>\n",
       "      <td>1.434847e-05</td>\n",
       "      <td>8.392926e-05</td>\n",
       "      <td>3.993694e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.494205e-07</td>\n",
       "      <td>2.626742e-07</td>\n",
       "      <td>5.958967e-07</td>\n",
       "      <td>3.694117e-08</td>\n",
       "      <td>2.893889e-07</td>\n",
       "      <td>4.808769e-06</td>\n",
       "      <td>9.190333e-09</td>\n",
       "      <td>1.631358e-07</td>\n",
       "      <td>2.373139e-04</td>\n",
       "      <td>1576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>3.328502e-06</td>\n",
       "      <td>4.548433e-06</td>\n",
       "      <td>1.768432e-08</td>\n",
       "      <td>1.867698e-05</td>\n",
       "      <td>8.773800e-08</td>\n",
       "      <td>2.391004e-08</td>\n",
       "      <td>2.897629e-07</td>\n",
       "      <td>2.707303e-05</td>\n",
       "      <td>1.263562e-04</td>\n",
       "      <td>3.186850e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>3.043790e-07</td>\n",
       "      <td>5.301507e-05</td>\n",
       "      <td>1.106488e-07</td>\n",
       "      <td>1.358890e-04</td>\n",
       "      <td>1.230335e-04</td>\n",
       "      <td>2.331171e-06</td>\n",
       "      <td>1.559184e-07</td>\n",
       "      <td>2.323380e-08</td>\n",
       "      <td>1.360111e-05</td>\n",
       "      <td>1577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>4.821304e-06</td>\n",
       "      <td>2.029359e-07</td>\n",
       "      <td>3.381831e-07</td>\n",
       "      <td>5.527019e-08</td>\n",
       "      <td>2.356026e-06</td>\n",
       "      <td>1.928449e-06</td>\n",
       "      <td>5.625118e-08</td>\n",
       "      <td>8.097131e-07</td>\n",
       "      <td>1.229190e-07</td>\n",
       "      <td>7.565694e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.086284e-08</td>\n",
       "      <td>4.788513e-06</td>\n",
       "      <td>1.491906e-06</td>\n",
       "      <td>1.700543e-07</td>\n",
       "      <td>9.782670e-10</td>\n",
       "      <td>2.159228e-09</td>\n",
       "      <td>2.067723e-07</td>\n",
       "      <td>3.558310e-08</td>\n",
       "      <td>2.091046e-05</td>\n",
       "      <td>1579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>6.769212e-05</td>\n",
       "      <td>1.772915e-05</td>\n",
       "      <td>1.028058e-03</td>\n",
       "      <td>1.094094e-04</td>\n",
       "      <td>1.925363e-04</td>\n",
       "      <td>8.442118e-06</td>\n",
       "      <td>3.324017e-04</td>\n",
       "      <td>7.797538e-04</td>\n",
       "      <td>3.727883e-05</td>\n",
       "      <td>2.969012e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.922325e-04</td>\n",
       "      <td>2.067485e-06</td>\n",
       "      <td>1.654053e-03</td>\n",
       "      <td>2.704636e-05</td>\n",
       "      <td>1.016871e-04</td>\n",
       "      <td>1.275333e-04</td>\n",
       "      <td>2.833566e-05</td>\n",
       "      <td>8.873074e-07</td>\n",
       "      <td>5.149308e-05</td>\n",
       "      <td>1580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>2.319233e-09</td>\n",
       "      <td>7.567194e-07</td>\n",
       "      <td>1.696412e-07</td>\n",
       "      <td>1.041737e-07</td>\n",
       "      <td>2.687386e-07</td>\n",
       "      <td>1.408885e-05</td>\n",
       "      <td>6.616469e-06</td>\n",
       "      <td>1.048376e-07</td>\n",
       "      <td>3.186373e-08</td>\n",
       "      <td>3.483736e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>9.348614e-09</td>\n",
       "      <td>2.026490e-07</td>\n",
       "      <td>3.675910e-08</td>\n",
       "      <td>7.338519e-07</td>\n",
       "      <td>4.351803e-07</td>\n",
       "      <td>5.240300e-08</td>\n",
       "      <td>2.001681e-07</td>\n",
       "      <td>1.336968e-08</td>\n",
       "      <td>2.069597e-07</td>\n",
       "      <td>1583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Acer_Capillipes  Acer_Circinatum     Acer_Mono   Acer_Opalus  \\\n",
       "0       9.168964e-06     1.136802e-05  1.058141e-07  4.820798e-05   \n",
       "1       1.404108e-07     3.778491e-08  3.627792e-07  4.197656e-06   \n",
       "2       6.556525e-07     9.934249e-01  2.928195e-06  4.801346e-07   \n",
       "3       3.353248e-06     1.768170e-02  5.160257e-06  3.126537e-06   \n",
       "4       2.034397e-06     8.090228e-05  7.313113e-08  2.690308e-08   \n",
       "..               ...              ...           ...           ...   \n",
       "589     6.380034e-07     9.980335e-01  6.156577e-06  1.200267e-07   \n",
       "590     3.328502e-06     4.548433e-06  1.768432e-08  1.867698e-05   \n",
       "591     4.821304e-06     2.029359e-07  3.381831e-07  5.527019e-08   \n",
       "592     6.769212e-05     1.772915e-05  1.028058e-03  1.094094e-04   \n",
       "593     2.319233e-09     7.567194e-07  1.696412e-07  1.041737e-07   \n",
       "\n",
       "     Acer_Palmatum   Acer_Pictum  Acer_Platanoids   Acer_Rubrum  \\\n",
       "0     3.256654e-06  4.137841e-05     7.403700e-08  3.047323e-06   \n",
       "1     1.302392e-07  2.827330e-07     1.338912e-05  5.901102e-07   \n",
       "2     3.973553e-03  5.642667e-06     5.375386e-07  3.463326e-05   \n",
       "3     1.798328e-05  1.564438e-06     1.352905e-04  1.044371e-04   \n",
       "4     1.299203e-05  3.201723e-07     1.299397e-05  1.374264e-05   \n",
       "..             ...           ...              ...           ...   \n",
       "589   4.377605e-04  1.970486e-07     1.943734e-07  1.434847e-05   \n",
       "590   8.773800e-08  2.391004e-08     2.897629e-07  2.707303e-05   \n",
       "591   2.356026e-06  1.928449e-06     5.625118e-08  8.097131e-07   \n",
       "592   1.925363e-04  8.442118e-06     3.324017e-04  7.797538e-04   \n",
       "593   2.687386e-07  1.408885e-05     6.616469e-06  1.048376e-07   \n",
       "\n",
       "     Acer_Rufinerve  Acer_Saccharinum  ...  Salix_Intergra   Sorbus_Aria  \\\n",
       "0      1.266115e-05      7.579653e-05  ...    1.759318e-04  4.958602e-06   \n",
       "1      4.014803e-07      5.012888e-07  ...    2.233165e-06  9.369525e-09   \n",
       "2      1.395726e-04      3.332193e-05  ...    2.319929e-07  4.067069e-07   \n",
       "3      2.254270e-04      2.053770e-04  ...    1.983198e-06  7.081412e-05   \n",
       "4      9.268543e-04      2.753391e-05  ...    1.143235e-06  1.649982e-05   \n",
       "..              ...               ...  ...             ...           ...   \n",
       "589    8.392926e-05      3.993694e-06  ...    2.494205e-07  2.626742e-07   \n",
       "590    1.263562e-04      3.186850e-08  ...    3.043790e-07  5.301507e-05   \n",
       "591    1.229190e-07      7.565694e-06  ...    1.086284e-08  4.788513e-06   \n",
       "592    3.727883e-05      2.969012e-06  ...    7.922325e-04  2.067485e-06   \n",
       "593    3.186373e-08      3.483736e-08  ...    9.348614e-09  2.026490e-07   \n",
       "\n",
       "     Tilia_Oliveri  Tilia_Platyphyllos  Tilia_Tomentosa  Ulmus_Bergmanniana  \\\n",
       "0     3.458318e-07        1.498040e-05     1.033900e-05        4.679465e-07   \n",
       "1     1.397477e-09        4.831081e-10     2.937486e-05        5.636082e-07   \n",
       "2     9.261282e-07        1.506777e-07     7.147002e-07        2.642378e-06   \n",
       "3     1.223778e-05        2.980604e-06     7.013277e-05        5.569041e-03   \n",
       "4     1.602802e-06        7.087561e-05     1.862322e-05        3.217035e-03   \n",
       "..             ...                 ...              ...                 ...   \n",
       "589   5.958967e-07        3.694117e-08     2.893889e-07        4.808769e-06   \n",
       "590   1.106488e-07        1.358890e-04     1.230335e-04        2.331171e-06   \n",
       "591   1.491906e-06        1.700543e-07     9.782670e-10        2.159228e-09   \n",
       "592   1.654053e-03        2.704636e-05     1.016871e-04        1.275333e-04   \n",
       "593   3.675910e-08        7.338519e-07     4.351803e-07        5.240300e-08   \n",
       "\n",
       "     Viburnum_Tinus  Viburnum_x_Rhytidophylloides  Zelkova_Serrata    id  \n",
       "0      1.301285e-07                  3.894677e-04     1.840167e-07     4  \n",
       "1      3.783148e-05                  2.054628e-07     1.343156e-07     7  \n",
       "2      1.349549e-08                  2.454829e-07     2.621117e-04     9  \n",
       "3      1.584182e-05                  1.661617e-06     8.732156e-04    12  \n",
       "4      5.068738e-06                  5.109041e-05     1.457895e-06    13  \n",
       "..              ...                           ...              ...   ...  \n",
       "589    9.190333e-09                  1.631358e-07     2.373139e-04  1576  \n",
       "590    1.559184e-07                  2.323380e-08     1.360111e-05  1577  \n",
       "591    2.067723e-07                  3.558310e-08     2.091046e-05  1579  \n",
       "592    2.833566e-05                  8.873074e-07     5.149308e-05  1580  \n",
       "593    2.001681e-07                  1.336968e-08     2.069597e-07  1583  \n",
       "\n",
       "[594 rows x 100 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_df = eval_model(model, x_test, sorted(df['species'].unique()), test_df['id'].values)\n",
    "k_df.to_csv(\"test_results.csv\", index=False)\n",
    "k_df"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the score\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import resnet50, VGG16 , InceptionV3, Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 30s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "vgg = VGG16(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=../input/keras-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5208/1243415216.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m conv_base = VGG16(weights='../input/keras-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n\u001b[0;32m      2\u001b[0m                   \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                   \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m                  )\n\u001b[0;32m      5\u001b[0m \u001b[0mconv_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\applications\\vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"imagenet\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         raise ValueError(\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[1;34m\"The `weights` argument should be either \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[1;34m\"`None` (random initialization), `imagenet` \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;34m\"(pre-training on ImageNet), \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=../input/keras-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"
     ]
    }
   ],
   "source": [
    "conv_base = VGG16(weights='../input/keras-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                  include_top=False, \n",
    "                  input_shape=(48, 48, 3)\n",
    "                 )\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False, \n",
    "                  input_shape=(32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "base = VGG16(weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "             include_top=False,\n",
    "             input_shape=(150,225,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 225, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 225, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 225, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 112, 64)       0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 112, 128)      73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 112, 128)      147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 14, 512)        0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 14, 512)        2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 14, 512)        2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 14, 512)        2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
